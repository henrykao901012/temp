{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b792f45b",
   "metadata": {},
   "source": [
    "# Two-Head MLP Training (TB Outcome + Confidence) â€” with Priors + Numba Feature Calls\n",
    "\n",
    "This notebook (v2):\n",
    "- Uses **generate_tb_labels** only (NO direction label)\n",
    "- Loads **3 prior feature files**: HMM / Markov / N-gram (parquet)\n",
    "- Computes **ret_1 / vol_20 / rsi_14** via your `features/numba_feats/*`\n",
    "- Trains a **true two-head MLP**\n",
    "  - Head A: TB **outcome** classification {-1,0,+1} (down / no-event / up)\n",
    "  - Head B: TB **confidence** regression (continuous TB, **hit-only** loss)\n",
    "- Evaluates event precision/recall, direction-on-event, ranking Precision@Top-K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Training / Evaluation Config (Embedded) =====\n",
    "\n",
    "DATA_SPLIT = {\n",
    "    \"train\": (\"2020-01-01\", \"2024-06-30\"),\n",
    "    \"val\": (\"2024-07-01\", \"2024-12-31\"),\n",
    "    \"test\": (\"2025-01-01\", \"2025-06-30\"),\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 50\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "CLIP_VALUE = 5.0\n",
    "\n",
    "# Two-head loss weight: total = CE(outcome) + LAMBDA_CONF * SmoothL1(conf)\n",
    "LAMBDA_CONF = 0.5\n",
    "\n",
    "HIDDEN_DIMS = [128, 128, 64]\n",
    "DROPOUT = 0.1\n",
    "\n",
    "BUCKETS = [-1.0, -0.5, -0.2, 0.2, 0.5, 1.0]\n",
    "\n",
    "# ===== Prior features (exactly the 3 files in your screenshot) =====\n",
    "PRIOR_FILES = {\n",
    "    \"hmm\": \"prior_features/hmm_features.parquet\",\n",
    "    \"markov\": \"prior_features/markov_features.parquet\",\n",
    "    \"ngram\": \"prior_features/ngram_features.parquet\",\n",
    "}\n",
    "\n",
    "# ===== Feature names we will build =====\n",
    "CONTINUOUS_COLS = [\"ret_1\", \"vol_20\", \"rsi_14\"]\n",
    "\n",
    "# For priors: auto-select columns by prefix.\n",
    "NGRAM_PREFIXES = (\"ng1_\", \"ng2_\", \"ng3_\")\n",
    "MARKOV_PREFIXES = (\"mk1_\", \"mk2_\", \"mk3_\")\n",
    "\n",
    "# HMM: either \"hmm_regime\" or \"hmm_state\" else any \"hmm_\" prefix columns\n",
    "HMM_REGIME_CANDIDATES = (\"hmm_regime\", \"hmm_state\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from labels.label_generator import generate_tb_labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6d90c",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Load Base DataFrame (time anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a0999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import load_kline_data\n",
    "\n",
    "data_file = Path(\n",
    "    r\"C:\\Users\\user\\Desktop\\binance-public-data-master\\python\\data\\spot\\monthly\\klines\\BTCUSDT\\5m\\BTCUSDT-5m-2020_to_2025_10.csv\"\n",
    ")\n",
    "\n",
    "df_base = load_kline_data(data_file).sort_index()\n",
    "\n",
    "print(\"Base df rows:\", len(df_base))\n",
    "print(\"Base df cols:\", len(df_base.columns))\n",
    "df_base.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501f6ed",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Load Priors (HMM / Markov / N-gram) and Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989646fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hmm = pd.read_parquet(PRIOR_FILES[\"hmm\"]).sort_index()\n",
    "df_mk = pd.read_parquet(PRIOR_FILES[\"markov\"]).sort_index()\n",
    "df_ng = pd.read_parquet(PRIOR_FILES[\"ngram\"]).sort_index()\n",
    "\n",
    "print(\"hmm:\", df_hmm.shape)\n",
    "print(\"markov:\", df_mk.shape)\n",
    "print(\"ngram:\", df_ng.shape)\n",
    "\n",
    "# Strict index-aligned join (no shift)\n",
    "df = df_base.join([df_hmm, df_mk, df_ng], how=\"inner\")\n",
    "print(\"Joined df rows:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4298347b",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Compute Continuous Features via your `features/numba_feats/*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a231ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from features.numba_feats.price_features import (\n",
    "    _compute_pct_return,\n",
    "    _compute_volatility,\n",
    ")\n",
    "from features.numba_feats.momentum_features import _compute_rsi\n",
    "\n",
    "close = df[\"Close\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "df[\"ret_1\"] = _compute_pct_return(close, 1).astype(np.float32)\n",
    "df[\"vol_20\"] = _compute_volatility(close, 20).astype(np.float32)\n",
    "df[\"rsi_14\"] = _compute_rsi(close, 14).astype(np.float32)\n",
    "\n",
    "CONTINUOUS_COLS = [\"ret_1\", \"vol_20\", \"rsi_14\"]\n",
    "\n",
    "# Train-only statistics\n",
    "train_start, train_end = DATA_SPLIT[\"train\"]\n",
    "train_mask = (df.index >= train_start) & (df.index <= train_end)\n",
    "\n",
    "train_df = df.loc[train_mask, CONTINUOUS_COLS]\n",
    "\n",
    "cont_mean = train_df.mean(skipna=True)\n",
    "cont_std = train_df.std(skipna=True).replace(0, np.nan)\n",
    "\n",
    "print(\"Train-only mean:\")\n",
    "print(cont_mean)\n",
    "print(\"\\nTrain-only std:\")\n",
    "print(cont_std)\n",
    "\n",
    "# Global z-score + clip [-5, 5]\n",
    "for c in CONTINUOUS_COLS:\n",
    "    df[c] = (df[c] - cont_mean[c]) / cont_std[c]\n",
    "    df[c] = df[c].clip(-5.0, 5.0)\n",
    "\n",
    "print(\"Continuous features after train-only zscore + clip:\")\n",
    "print(df[CONTINUOUS_COLS].describe().T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16656480",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Generate TB Labels (Outcome + Confidence)\n",
    "\n",
    "- Outcome (classification): `tb.labels âˆˆ {-1,0,+1}` mapped to `{0,1,2}` for CE\n",
    "- Confidence (regression): continuous TB value; **loss is hit-only** (outcome != 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = generate_tb_labels(df)\n",
    "\n",
    "# tb.labels is expected to be a pandas Series aligned to df.index\n",
    "tb_label = tb.labels.astype(np.float32)\n",
    "\n",
    "# Outcome class for CE: {-1,0,+1} -> {0,1,2}\n",
    "y_outcome = (tb_label.values.astype(int) + 1).astype(np.int64)\n",
    "\n",
    "# Confidence / intensity target (continuous TB)\n",
    "y_conf = tb_label.values.astype(np.float32)\n",
    "\n",
    "print(\"TB outcome distribution (0=down,1=no-event,2=up):\")\n",
    "vals, cnts = np.unique(y_outcome, return_counts=True)\n",
    "print(dict(zip(vals, cnts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0096b3f",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Align & Drop NaNs\n",
    "\n",
    "- keep only rows where continuous features exist\n",
    "- TB labels are always aligned (same index), confidence loss will be masked hit-only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b609343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align TB series to df index (safety)\n",
    "tb_label = tb_label.loc[df.index]\n",
    "\n",
    "mask = pd.Series(True, index=df.index)\n",
    "for c in CONTINUOUS_COLS:\n",
    "    mask &= ~df[c].isna()\n",
    "\n",
    "df = df.loc[mask]\n",
    "tb_label = tb_label.loc[mask]\n",
    "\n",
    "y_outcome = (tb_label.values.astype(int) + 1).astype(np.int64)\n",
    "y_conf = tb_label.values.astype(np.float32)\n",
    "\n",
    "print(\"After alignment:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a9a65",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Select Prior Columns for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram columns\n",
    "ng_cols = [c for c in df.columns if c.startswith(NGRAM_PREFIXES)]\n",
    "\n",
    "# Markov columns\n",
    "mk_cols = [c for c in df.columns if c.startswith(MARKOV_PREFIXES)]\n",
    "\n",
    "# HMM columns\n",
    "hmm_cols = []\n",
    "for cand in HMM_REGIME_CANDIDATES:\n",
    "    if cand in df.columns:\n",
    "        hmm_cols = [cand]\n",
    "        break\n",
    "if not hmm_cols:\n",
    "    hmm_cols = [c for c in df.columns if c.startswith(\"hmm_\")]\n",
    "\n",
    "print(\"Selected columns:\")\n",
    "print(\"  ng_cols:\", len(ng_cols))\n",
    "print(\"  mk_cols:\", len(mk_cols))\n",
    "print(\"  hmm_cols:\", len(hmm_cols), hmm_cols[:10])\n",
    "\n",
    "if len(ng_cols) == 0:\n",
    "    raise ValueError(\"No ngram columns found. Check ngram_features.parquet column names.\")\n",
    "if len(mk_cols) == 0:\n",
    "    raise ValueError(\"No markov columns found. Check markov_features.parquet column names.\")\n",
    "if len(hmm_cols) == 0:\n",
    "    raise ValueError(\"No hmm columns found. Check hmm_features.parquet column names.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4508b8",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Time-based Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c706008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_df(d, start, end):\n",
    "    return d.loc[(d.index >= start) & (d.index <= end)]\n",
    "\n",
    "df_train = slice_df(df, *DATA_SPLIT[\"train\"])\n",
    "df_val = slice_df(df, *DATA_SPLIT[\"val\"])\n",
    "df_test = slice_df(df, *DATA_SPLIT[\"test\"])\n",
    "\n",
    "# Targets aligned by index\n",
    "tb_train_s = tb_label.loc[df_train.index]\n",
    "tb_val_s = tb_label.loc[df_val.index]\n",
    "tb_test_s = tb_label.loc[df_test.index]\n",
    "\n",
    "y_outcome_train = (tb_train_s.values.astype(int) + 1).astype(np.int64)\n",
    "y_outcome_val = (tb_val_s.values.astype(int) + 1).astype(np.int64)\n",
    "y_outcome_test = (tb_test_s.values.astype(int) + 1).astype(np.int64)\n",
    "\n",
    "y_conf_train = tb_train_s.values.astype(np.float32)\n",
    "y_conf_val = tb_val_s.values.astype(np.float32)\n",
    "y_conf_test = tb_test_s.values.astype(np.float32)\n",
    "\n",
    "print(\"Train rows:\", len(df_train), \"Val rows:\", len(df_val), \"Test rows:\", len(df_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7afa114",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Feature-wise Preprocessing (NO global z-score)\n",
    "\n",
    "- Continuous: clip + zscore using **train stats only**\n",
    "- N-gram / Markov probabilities: clip to [0,1]\n",
    "- Other Markov / N-gram scalars: clip + zscore (train-only)\n",
    "- HMM discrete regime: keep as float32 (no zscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify prob-like columns in priors\n",
    "prob_suffixes = (\"_p_up\", \"_p_down\", \"_p_flat\", \"_maxprob\")\n",
    "ng_prob_cols = [c for c in ng_cols if c.endswith(prob_suffixes)]\n",
    "mk_prob_cols = [c for c in mk_cols if c.endswith(prob_suffixes)]\n",
    "\n",
    "# Scalar columns in priors\n",
    "ng_scalar_cols = [c for c in ng_cols if c not in ng_prob_cols]\n",
    "mk_scalar_cols = [c for c in mk_cols if c not in mk_prob_cols]\n",
    "\n",
    "DISCRETE_COLS = hmm_cols\n",
    "\n",
    "def clip_arr(x, lo, hi):\n",
    "    return np.clip(x, lo, hi)\n",
    "\n",
    "# continuous train stats\n",
    "Xc_train = clip_arr(df_train[CONTINUOUS_COLS].values.astype(np.float32), -CLIP_VALUE, CLIP_VALUE)\n",
    "c_mean = Xc_train.mean(0, keepdims=True)\n",
    "c_std = Xc_train.std(0, keepdims=True) + 1e-8\n",
    "\n",
    "# scalar priors train stats\n",
    "Xs_train = pd.concat([df_train[ng_scalar_cols], df_train[mk_scalar_cols]], axis=1).values.astype(np.float32)\n",
    "Xs_train = clip_arr(Xs_train, -CLIP_VALUE, CLIP_VALUE)\n",
    "s_mean = Xs_train.mean(0, keepdims=True)\n",
    "s_std = Xs_train.std(0, keepdims=True) + 1e-8\n",
    "\n",
    "def prep_cont(d):\n",
    "    x = clip_arr(d[CONTINUOUS_COLS].values.astype(np.float32), -CLIP_VALUE, CLIP_VALUE)\n",
    "    return (x - c_mean) / c_std\n",
    "\n",
    "def prep_prob(d, cols):\n",
    "    return np.clip(d[cols].values.astype(np.float32), 0.0, 1.0)\n",
    "\n",
    "def prep_scalar(d):\n",
    "    x = pd.concat([d[ng_scalar_cols], d[mk_scalar_cols]], axis=1).values.astype(np.float32)\n",
    "    x = clip_arr(x, -CLIP_VALUE, CLIP_VALUE)\n",
    "    return (x - s_mean) / s_std\n",
    "\n",
    "def prep_disc(d):\n",
    "    return d[DISCRETE_COLS].values.astype(np.float32)\n",
    "\n",
    "def build_X(d):\n",
    "    return np.hstack([\n",
    "        prep_cont(d),\n",
    "        prep_prob(d, ng_prob_cols),\n",
    "        prep_prob(d, mk_prob_cols),\n",
    "        prep_scalar(d),\n",
    "        prep_disc(d),\n",
    "    ])\n",
    "\n",
    "X_train = build_X(df_train)\n",
    "X_val = build_X(df_val)\n",
    "X_test = build_X(df_test)\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_val:\", X_val.shape, \"X_test:\", X_test.shape)\n",
    "print(\"outcome classes train:\", np.unique(y_outcome_train, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5819a0e",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Dataset / Dataloader (two targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoHeadDS(Dataset):\n",
    "    def __init__(self, X, y_outcome, y_conf):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_outcome = torch.tensor(y_outcome, dtype=torch.long)\n",
    "        self.y_conf = torch.tensor(y_conf, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y_outcome[i], self.y_conf[i]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TwoHeadDS(X_train, y_outcome_train, y_conf_train),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TwoHeadDS(X_val, y_outcome_val, y_conf_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1d6ab",
   "metadata": {},
   "source": [
    "## ðŸ”Ÿ Two-Head MLP (shared backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403341a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoHeadMLP(nn.Module):\n",
    "    def __init__(self, d_in, hidden_dims=(128, 128, 64), dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = d_in\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(d, h))\n",
    "            layers.append(nn.GELU())\n",
    "            if dropout and dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            d = h\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        self.head_outcome = nn.Linear(d, 3)  # 0=down,1=no-event,2=up\n",
    "        self.head_conf = nn.Linear(d, 1)     # continuous TB confidence\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        logits_outcome = self.head_outcome(h)\n",
    "        pred_conf = self.head_conf(h).squeeze(-1)\n",
    "        return logits_outcome, pred_conf\n",
    "\n",
    "device = DEVICE if (DEVICE == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
    "model = TwoHeadMLP(X_train.shape[1], hidden_dims=tuple(HIDDEN_DIMS), dropout=DROPOUT).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Weight no-event class lower; emphasize events\n",
    "loss_outcome_fn = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 0.4, 1.0], device=device))\n",
    "loss_conf_fn = nn.SmoothL1Loss()\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(\"Model params:\", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2ea10",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£1ï¸âƒ£ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    o_sum = 0.0\n",
    "    c_sum = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for x, y_o, y_c in train_loader:\n",
    "        x = x.to(device)\n",
    "        y_o = y_o.to(device)\n",
    "        y_c = y_c.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        logits_o, pred_conf = model(x)\n",
    "\n",
    "        # Outcome loss (always)\n",
    "        loss_o = loss_outcome_fn(logits_o, y_o)\n",
    "\n",
    "        # Confidence loss (hit-only: outcome != no-event (class 1))\n",
    "        hit_mask = (y_o != 1)\n",
    "        if hit_mask.any():\n",
    "            loss_c = loss_conf_fn(pred_conf[hit_mask], y_c[hit_mask])\n",
    "        else:\n",
    "            loss_c = torch.tensor(0.0, device=device)\n",
    "\n",
    "        loss = loss_o + LAMBDA_CONF * loss_c\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        o_sum += float(loss_o.item())\n",
    "        c_sum += float(loss_c.item())\n",
    "        n_batches += 1\n",
    "\n",
    "    if ep % 5 == 0 or ep == 1:\n",
    "        print(f\"Epoch {ep:02d} | outcome_loss={o_sum/n_batches:.4f} | conf_loss={c_sum/n_batches:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a4279",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£2ï¸âƒ£ Evaluation (Event / Direction-on-Event / Ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_o, pred_conf = model(X_test_t.to(device))\n",
    "\n",
    "pred_outcome = logits_o.argmax(1).cpu().numpy()  # {0,1,2}\n",
    "true_outcome = y_outcome_test                    # {0,1,2}\n",
    "\n",
    "print(\"=== Outcome Head ===\")\n",
    "print(\"Outcome accuracy:\", accuracy_score(true_outcome, pred_outcome))\n",
    "\n",
    "event_true = (true_outcome != 1)\n",
    "event_pred = (pred_outcome != 1)\n",
    "\n",
    "tp = int((event_true & event_pred).sum())\n",
    "pp = int(event_pred.sum())\n",
    "ap = int(event_true.sum())\n",
    "\n",
    "prec = tp / max(pp, 1)\n",
    "rec = tp / max(ap, 1)\n",
    "\n",
    "print(\"Event precision:\", prec)\n",
    "print(\"Event recall   :\", rec)\n",
    "\n",
    "if ap > 0:\n",
    "    dir_acc_event = (pred_outcome[event_true] == true_outcome[event_true]).mean()\n",
    "else:\n",
    "    dir_acc_event = float(\"nan\")\n",
    "print(\"Direction acc | event:\", dir_acc_event)\n",
    "\n",
    "print(\"\\n=== Ranking / Precision@Top-K (by |pred_conf|) ===\")\n",
    "pred_conf_np = pred_conf.cpu().numpy().astype(np.float32)\n",
    "abs_pred = np.abs(pred_conf_np)\n",
    "order = np.argsort(-abs_pred)\n",
    "\n",
    "base_event_rate = event_true.mean()\n",
    "print(\"Base event rate:\", float(base_event_rate))\n",
    "\n",
    "for k in [100, 500, 1000]:\n",
    "    if k > len(order):\n",
    "        continue\n",
    "    idx = order[:k]\n",
    "    topk_event_prec = event_true[idx].mean()\n",
    "    print(f\"Top-{k:4d} | Event Precision={topk_event_prec:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
